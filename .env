# Shared Ollama Service Environment Configuration
# Copy this file to .env and customize for your environment
# All variables are optional and have sensible defaults

# ============================================================================
# Ollama Service Configuration
# ============================================================================
# Base URL for Ollama service (overrides OLLAMA_HOST and OLLAMA_PORT if set)
# OLLAMA_BASE_URL=http://localhost:11434

# Ollama service host and port
OLLAMA_PORT=11434

# Ollama server configuration
OLLAMA_DEBUG=false
OLLAMA_ORIGINS=*

# Apple Silicon MPS/Metal GPU optimization
# OLLAMA_NUM_GPU=999        # Alternative: explicitly set high value to use all cores

# CPU optimization (optional - auto-detected if not set)
# OLLAMA_NUM_THREAD=10      # Auto-detected based on CPU cores

# Memory optimization (automatically calculated based on system RAM)
# Calculate optimal value: ./scripts/calculate_memory_limit.sh
# Formula: Total RAM - 25% (minimum 8GB reserved for system)
# Example: 32GB RAM → 24GB for Ollama, 8GB for system
# Example: 16GB RAM → 12GB for Ollama, 4GB for system
# OLLAMA_MAX_RAM=24GB       # Auto-calculated (run calculate_memory_limit.sh to get value)
# OLLAMA_NUM_PARALLEL=2     # Allow 2 models to run in parallel (if needed)

# ============================================================================
# API Server Configuration
# ============================================================================
API_PORT=8000
API_RELOAD=false                    # Enable auto-reload (development only)
API_LOG_LEVEL=info                  # debug, info, warning, error, critical
API_TITLE=Shared Ollama Service API
API_VERSION=2.0.0
API_DOCS_URL=/api/docs
API_OPENAPI_URL=/api/openapi.json

# ============================================================================
# Request Queue Configuration
# ============================================================================
# Chat queue settings
QUEUE_CHAT_MAX_CONCURRENT=6         # Max concurrent chat requests (1-100)
QUEUE_CHAT_MAX_QUEUE_SIZE=50        # Max chat queue depth (1-1000)
QUEUE_CHAT_DEFAULT_TIMEOUT=60.0     # Default chat timeout in seconds (1.0-600.0)

# VLM queue settings (more resource-intensive)
QUEUE_VLM_MAX_CONCURRENT=3          # Max concurrent VLM requests (1-50)
QUEUE_VLM_MAX_QUEUE_SIZE=20        # Max VLM queue depth (1-500)
QUEUE_VLM_DEFAULT_TIMEOUT=120.0     # Default VLM timeout in seconds (1.0-1200.0)

# ============================================================================
# Batch Processing Configuration
# ============================================================================
BATCH_CHAT_MAX_CONCURRENT=5         # Max concurrent batch chat requests (1-50)
BATCH_CHAT_MAX_REQUESTS=50          # Max requests per batch chat (1-1000)
BATCH_VLM_MAX_CONCURRENT=3          # Max concurrent batch VLM requests (1-20)
BATCH_VLM_MAX_REQUESTS=20           # Max requests per batch VLM (1-500)

# ============================================================================
# Image Processing Configuration (for VLM)
# ============================================================================
IMAGE_MAX_DIMENSION=1024            # Max image dimension in pixels (256-2048)
IMAGE_JPEG_QUALITY=85               # JPEG compression quality (1-100)
IMAGE_PNG_COMPRESSION=6             # PNG compression level (0-9)
IMAGE_MAX_SIZE_BYTES=10485760        # Max image size in bytes (10MB default, 1MB-100MB)

# ============================================================================
# Image Cache Configuration
# ============================================================================
IMAGE_CACHE_MAX_SIZE=100            # Max cached images (1-10000)
IMAGE_CACHE_TTL_SECONDS=3600.0      # Cache TTL in seconds (60.0-86400.0)

# ============================================================================
# Async Client Configuration
# ============================================================================
CLIENT_TIMEOUT=300                  # Request timeout in seconds (1-3600)
CLIENT_HEALTH_CHECK_TIMEOUT=5      # Health check timeout in seconds (1-60)
CLIENT_MAX_CONNECTIONS=50           # Max HTTP connections (1-1000)
CLIENT_MAX_KEEPALIVE_CONNECTIONS=20 # Max keep-alive connections (1-500)
CLIENT_MAX_CONCURRENT_REQUESTS=     # Max concurrent requests (None = unlimited, 1+)
CLIENT_MAX_RETRIES=3                # Max retry attempts (0-10)
CLIENT_RETRY_DELAY=1.0              # Retry delay in seconds (0.1-10.0)
CLIENT_VERBOSE=false                # Verbose logging

# ============================================================================
# Ollama Manager Configuration
# ============================================================================
OLLAMA_MANAGER_AUTO_DETECT_OPTIMIZATIONS=true  # Auto-detect system optimizations
OLLAMA_MANAGER_WAIT_FOR_READY=true             # Wait for service to be ready on start
OLLAMA_MANAGER_MAX_WAIT_TIME=30                 # Max wait time for readiness in seconds (1-300)
OLLAMA_MANAGER_SHUTDOWN_TIMEOUT=10              # Shutdown timeout in seconds (1-60)

# ============================================================================
# Notes
# ============================================================================
# - Metal/MPS acceleration is automatic on Apple Silicon, but explicitly setting
#   OLLAMA_METAL=1 ensures maximum GPU utilization for best performance
#
# - All numeric values have validation ranges - see src/shared_ollama/core/config.py
#   for exact limits and descriptions
#
# - Configuration is loaded from environment variables with the prefixes shown above
#   (e.g., API_HOST, QUEUE_CHAT_MAX_CONCURRENT, etc.)
#
# - The .env file is automatically loaded if present in the project root
#
# - For production, set appropriate values based on your system resources and
#   expected load patterns

# ============================================================================
# Auto-Generated Optimal Configuration
# Generated on: Wed Nov 19 16:15:37 GMT 2025
# System: M4 Pro (arm64) - 64GB RAM, 14 CPU cores
# ============================================================================

# Metal GPU acceleration (Apple Silicon only)
OLLAMA_METAL=1

# Number of GPU cores (-1 = all available)
OLLAMA_NUM_GPU=-1

# CPU threads (auto-detected: 14 cores)
OLLAMA_NUM_THREAD=14

# Maximum RAM for Ollama (calculated: 64GB total - 25% system reserve)
OLLAMA_MAX_RAM=44GB

# Number of parallel models (calculated based on available RAM)
OLLAMA_NUM_PARALLEL=3

# Model keep-alive duration (30m recommended for better performance)
OLLAMA_KEEP_ALIVE=30m

# Ollama service host (0.0.0.0 = network accessible, localhost = local only)
OLLAMA_HOST=localhost

# REST API host (0.0.0.0 = network accessible, localhost = local only)
API_HOST=0.0.0.0

